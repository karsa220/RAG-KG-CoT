#!/usr/bin/env python3
# history_hotspots_rag_glm.py

import os
import json
import time
import requests
from typing import List, Dict, Any, Tuple
from dotenv import load_dotenv

# Embedding / vector search
try:
    from sentence_transformers import SentenceTransformer
    import faiss
    import numpy as np
    REAL_EMBED = True
except:
    REAL_EMBED = False

# ----------------------------
# 1. Load ZhipuAI client
# ----------------------------
from zai import ZhipuAiClient
load_dotenv()
ZHIPU_API_KEY = os.getenv("ZHIPU_API_KEY")

if not ZHIPU_API_KEY:
    raise RuntimeError("è¯·åœ¨ .env ä¸­è®¾ç½® ZHIPU_API_KEYï¼")

client = ZhipuAiClient(api_key=ZHIPU_API_KEY)

# ----------------------------
# 2. Mock dataset (fallback)
# ----------------------------
MOCK_PAPERS = [
    {
        "id": "m1",
        "title": "OCR Correction for Historical Documents",
        "abstract": "We propose LM-based post-processing to reduce OCR character error rates across 19th-century scanned texts.",
        "year": 2024,
        "source": "mock"
    },
    {
        "id": "m2",
        "title": "Social Networks of Qing Dynasty",
        "abstract": "We build networks from archival correspondence to study political influence structures.",
        "year": 2023,
        "source": "mock"
    },
    {
        "id": "m3",
        "title": "NER for Gazetteers",
        "abstract": "Applying NER to historical gazetteers helps extract place-names and administrative units.",
        "year": 2024,
        "source": "mock"
    }
]

# ----------------------------
# 3. OpenAlex Fetcher
# ----------------------------
def fetch_openalex(query: str, max_results=200):
    base = "https://api.openalex.org/works"
    params = {
        "search": query,
        "filter": "from_publication_date:2023-01-01",
        "per-page": 200
    }

    r = requests.get(base, params=params, timeout=10)
    data = r.json()
    works = data.get("results", [])

    out = []
    for w in works[:max_results]:
        abstract = None
        if w.get("abstract") is not None:
            abstract = w["abstract"]
        elif w.get("abstract_inverted_index") is not None:
            inv = w["abstract_inverted_index"]
            max_pos = max([max(v) for v in inv.values()])
            tokens = [""] * (max_pos + 1)
            for tok, idxs in inv.items():
                for i in idxs:
                    tokens[i] = tok
            abstract = " ".join(tokens)

        out.append({
            "id": w["id"],
            "title": w["title"],
            "abstract": abstract or "",
            "year": w.get("publication_year"),
            "source": "openalex"
        })

    return out


# ----------------------------
# 4. Retriever
# ----------------------------
class Retriever:
    def __init__(self):
        if REAL_EMBED:
            self.model = SentenceTransformer("all-MiniLM-L6-v2")
        else:
            self.model = None
        self.index = None
        self.ids = []
        self.corpus = []
        self.docs = {}

    def build(self, docs):
        self.docs = {d["id"]: d for d in docs}
        self.ids = [d["id"] for d in docs]
        self.corpus = [
            d["title"] + ". " + (d["abstract"] or "")
            for d in docs
        ]

        if REAL_EMBED:
            print("[INFO] Building FAISS index...")
            emb = self.model.encode(self.corpus, convert_to_tensor=False)
            emb = np.array(emb).astype("float32")
            faiss.normalize_L2(emb)
            d = emb.shape[1]
            self.index = faiss.IndexFlatIP(d)
            self.index.add(emb)
            self.emb_matrix = emb
        else:
            print("[WARN] sentence-transformers unavailable â†’ fallback keyword search.")
            self.index = None

    def search(self, query, k=20):
        if self.index is None:
            # fallback: keyword match
            qt = set(query.lower().split())
            scored = []
            for i, txt in enumerate(self.corpus):
                score = len(qt & set(txt.lower().split()))
                scored.append((self.ids[i], score))
            return sorted(scored, key=lambda x: x[1], reverse=True)[:k]

        # real embedding search
        q_emb = self.model.encode([query], convert_to_tensor=False)
        q_emb = np.array(q_emb).astype("float32")
        faiss.normalize_L2(q_emb)
        D, I = self.index.search(q_emb, k)
        out = []
        for idx, score in zip(I[0], D[0]):
            out.append((self.ids[idx], float(score)))
        return out


# ----------------------------
# 5. ZhipuAI Chat wrapper
# ----------------------------
def chat_with_glm(prompt: str) -> str:
    """
    ä½¿ç”¨ GLM-4.5-FLASH è¿›è¡Œ RAG æ€»ç»“
    """
    print("[INFO] è°ƒç”¨ GLM æ¨¡å‹ç”Ÿæˆæ€»ç»“ï¼ˆæµå¼è¾“å‡ºï¼‰...\n")

    response = client.chat.completions.create(
        model="glm-4.5-flash",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€åå†å²å­¦ç ”ç©¶è¶‹åŠ¿åˆ†æä¸“å®¶ã€‚"},
            {"role": "user", "content": prompt}
        ],
        stream=True,
        thinking={"type": "enabled"},
        max_tokens=2048,
        temperature=0.2
    )

    final_text = ""

    for chunk in response:
        delta = chunk.choices[0].delta.content
        if delta:
            print(delta, end="", flush=True)
            final_text += delta

    print("\n\n[INFO] GLM æ€»ç»“å®Œæˆã€‚\n")
    return final_text


# ----------------------------
# 6. Full RAG pipeline
# ----------------------------
def run_pipeline(query: str, use_openalex=False):
    print("\n=========== ğŸ§  å†å²å­¦çƒ­ç‚¹ RAG ç³»ç»Ÿ ===========\n")

    # Step 1: fetch real or mock papers
    if use_openalex:
        print("[INFO] æ­£åœ¨ä» OpenAlex æ‹‰å–çœŸå®è®ºæ–‡...")
        docs = fetch_openalex(query)
        if not docs:
            print("[WARN] æ— æ³•ä» OpenAlex è·å–ï¼Œfallback åˆ° mock")
            docs = MOCK_PAPERS
    else:
        docs = MOCK_PAPERS

    print(f"[INFO] æ–‡çŒ®æ•°é‡ï¼š{len(docs)}")

    # Step 2: build retriever
    r = Retriever()
    r.build(docs)

    # Step 3: search
    hits = r.search(query, k=20)
    print(f"[INFO] æ£€ç´¢åˆ° {len(hits)} æ¡æ–‡çŒ®ã€‚")

    # Step 4: assemble text for LLM
    evidence = []
    for pid, score in hits:
        doc = r.docs[pid]
        snippet = doc["title"] + "\n" + doc["abstract"][:500]
        evidence.append(snippet)

    evidence_text = "\n\n".join(evidence)

    # Step 5: call GLM
    PROMPT = f"""
è¯·æ ¹æ®ä»¥ä¸‹æœ€æ–°å†å²å­¦æ–‡çŒ®ï¼ˆå‡ä¸º2023-2025å¹´ï¼‰å†…å®¹ï¼Œåˆ†æã€Œå†å²å­¦æœ€æ–°ç ”ç©¶çƒ­ç‚¹ã€ã€‚

æ–‡çŒ®åˆ—è¡¨ï¼š
{evidence_text}

è¯·è¾“å‡ºï¼š
1. ä¸€ä¸ªå¯¹å†å²å­¦é¢†åŸŸè¿‡å»ä¸¤å¹´ï¼ˆ2023â€“2025ï¼‰çš„æ•´ä½“è¶‹åŠ¿æ€»ç»“  
2. äº”å¤§ç ”ç©¶çƒ­ç‚¹ï¼ˆæ¯ä¸ªçƒ­ç‚¹ 2 å¥è§£é‡Šï¼‰  
3. æ¯ä¸ªçƒ­ç‚¹è‡³å°‘åˆ—å‡ºä¸¤ç¯‡ä»£è¡¨è®ºæ–‡ï¼ˆæ ‡é¢˜ + å¹´ä»½ï¼‰  
4. ç»™å‡ºä¸‰ä¸ªæœªæ¥å¯ç ”ç©¶æ–¹å‘ï¼ˆå¯ä½œä¸ºç§‘ç ”é€‰é¢˜ï¼‰

è¯·ç”¨æ¸…æ™°ç»“æ„åŒ–æ ¼å¼å›ç­”ã€‚
"""

    summary = chat_with_glm(PROMPT)

    # Step 6: return
    return {
        "query": query,
        "llm_output": summary,
        "retrieved_papers": hits
    }


# ----------------------------
# CLI
# ----------------------------
def main():
    print("=== å†å²å­¦çƒ­ç‚¹æ£€ç´¢åŠ©æ‰‹ï¼ˆRAG + GLMï¼‰ ===")
    query = input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆä¾‹å¦‚ï¼šå†å²å­¦ æœ€æ–° ç ”ç©¶ çƒ­ç‚¹ï¼‰:\n> ").strip()
    if not query:
        query = "å†å²å­¦ æœ€æ–° ç ”ç©¶ çƒ­ç‚¹"

    out = run_pipeline(query, use_openalex=False)

    with open("history_rag_output.json", "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)

    print("\nç»“æœå·²ä¿å­˜åˆ° history_rag_output.json\n")


if __name__ == "__main__":
    main()
